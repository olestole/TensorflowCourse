{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k0ukKSNEi_0y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPnTeh6ki_0z",
        "outputId": "c2de21c0-9c16-4581-884d-3277843acfbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Will save the file to .keras\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdArmJmEi_00",
        "outputId": "1d54ea34-1380-42e7-960e-c568b377c39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file).read()\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U-GePaoi_01",
        "outputId": "f63c630e-30d1-4cf8-9b53-89e71fd9d226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# Since we're gonna train on characters, get the unique characters of the text\n",
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fK8arlZki_02"
      },
      "outputs": [],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1VQ7TFPi_03",
        "outputId": "4f6986eb-8e9f-484e-9d4c-d6382699cca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n",
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
          ]
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "chars = chars_from_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbk6Brg0i_03",
        "outputId": "fc87830c-7012-4c7f-d9c9-3f8e862b25c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'abcdefg'\n"
          ]
        }
      ],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "print(text_from_ids(ids).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0aiv5DCQi_04"
      },
      "outputs": [],
      "source": [
        "# Training model to  predict following:\n",
        "# Given a character, or a sequence of characters, what is the most probable next character?\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37DAFGqi_04",
        "outputId": "b71fab43-9e89-4b3f-ae4f-391d20953212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GNAFHiYWi_04"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W5HDcHVi_05",
        "outputId": "b8b893af-2c11-4398-e62b-784ccb1f536b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl1vXAp0i_05",
        "outputId": "3931b4d0-0bdc-483c-92e6-087df51d6112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create dataset of (input, label)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owFumviBi_06",
        "outputId": "040aa921-4636-4d48-83e2-09f33ab8d9e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input :\t b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target:\t b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "# Split all the sequences in sequences\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\\t\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\\t\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scm4YLnoi_08",
        "outputId": "5d1ed744-0bd4-4d86-8746-5139717816b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ],
      "source": [
        "# Will create batches of batch_size and shuffle them buffer_size at a time\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR6tH8U-i_09"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ICJAQpRTi_0_"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIBw5bc-i_0_",
        "outputId": "bc9b62ec-82b5-4be7-890d-473726e6e4ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp1Sih-qi_1A",
        "outputId": "6a763af2-5274-4983-f595-f5167abe726e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'ambush of my name, strike home,\\nAnd yet my nature never in the fight\\nTo do in slander. And to behold' \n",
            "\n",
            "Next Char Predictions:\n",
            " b\"rq!jl:NlBpnmjPGDXmXd[UNK]yaEla$[UNK]ENtzaHdsI:uPMS-,$wMaLJZ:&D.wOgBiHmv[UNK]iaakfZeoBz.[UNK]RT3Rm!nQJ'iIUhCkZHBd'djs\"\n"
          ]
        }
      ],
      "source": [
        "# Testing the untrained model\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "sampled_indices\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy(), \"\\n\")\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG2VwriYi_1B",
        "outputId": "aa72d883-6062-482b-aef9-7c0ec1bb95e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.188535\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch,example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9O7NChsi_1B",
        "outputId": "376290f9-b3b4-41ea-b25b-1d1abd5a5c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65.926155\n"
          ]
        }
      ],
      "source": [
        "# The untrained model shouldn't be too sure of itself -> Get approx vocab size\n",
        "print(tf.exp(mean_loss).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dopwzEsVi_1C"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpKahvhKi_1C",
        "outputId": "ca26aca4-f8bf-4c12-fffc-e6703adef5ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '../data/C3-RNNShakespeareCharacter'\n"
          ]
        }
      ],
      "source": [
        "checkpoint_dir = \"../data/C3-RNNShakespeareCharacter\"\n",
        "try:\n",
        "    os.mkdir(checkpoint_dir)\n",
        "except OSError as e:\n",
        "    print(e)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RtEexXOvi_1C"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath = checkpoint_prefix,\n",
        "  save_weights_only = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgC1HRSxi_1D",
        "outputId": "bd45ff64-8963-423e-95bb-992e4e24dcec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 26s 127ms/step - loss: 2.7349 - accuracy: 0.2740\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 2.0000 - accuracy: 0.4154\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.7165 - accuracy: 0.4907\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 1.5525 - accuracy: 0.5343\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 1.4525 - accuracy: 0.5601\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 1.3839 - accuracy: 0.5775\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 1.3309 - accuracy: 0.5905\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 26s 144ms/step - loss: 1.2855 - accuracy: 0.6021\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 29s 159ms/step - loss: 1.2453 - accuracy: 0.6122\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 27s 147ms/step - loss: 1.2048 - accuracy: 0.6229\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 1.1646 - accuracy: 0.6343\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 1.1242 - accuracy: 0.6454\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 23s 128ms/step - loss: 1.0818 - accuracy: 0.6574\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 24s 127ms/step - loss: 1.0352 - accuracy: 0.6715\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 23s 128ms/step - loss: 0.9866 - accuracy: 0.6867\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 0.9363 - accuracy: 0.7024\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 23s 127ms/step - loss: 0.8842 - accuracy: 0.7188\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 23s 128ms/step - loss: 0.8322 - accuracy: 0.7359\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 24s 128ms/step - loss: 0.7790 - accuracy: 0.7531\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 24s 130ms/step - loss: 0.7305 - accuracy: 0.7689\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ApSMDRe1i_1D"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ipjp50jUlLoc"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ziH0jzylM_S",
        "outputId": "dbd08e7d-0e99-4fe5-bee4-e853922cd7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JULIET:\n",
            "\n",
            "VOLUMNIA:\n",
            "Away with him to seven. thou must needs conqueron.\n",
            "\n",
            "COMINIUS:\n",
            "What's these I 'He's a sweeter hate.\n",
            "\n",
            "KING RICHARD II:\n",
            "So--Nor I. I'll open the house of Lancas.\n",
            "Ay, what should you sleep how to deny him, princely gentleman\n",
            "Wherein your party have and rotten sweet:\n",
            "I had sworn thus construe that thy bosomily sun.\n",
            "Under my lord, I'll woo her, by the hard way hands upon.\n",
            "Why art, come? not the Duke of Gloucester?\n",
            "\n",
            "Lord Mayor:\n",
            "Was not Saints have we spent intolting witness\n",
            "Are spaced beat to part them in the traitor, justify in prover\n",
            "Turn to die, And loved this, would you have to knowledge\n",
            "Of his master's blood, when you have deserved thee\n",
            "and thus may be angry but that were words:\n",
            "Besides, the people did prefer thee\n",
            "Let him deserve to honour and his knife,\n",
            "Whose haughty, bleed in all, best rabst.\n",
            "\n",
            "WARWICK:\n",
            "Find you so shall you hear ha's yet no more deeds?\n",
            "For you, nom government that you did;\n",
            "Thou shouldst eleven in a maiden man\n",
            "Is not a sense of him that hath the reart hudged \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.8679187297821045\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['JULIET:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig7x5E-ArzZY",
        "outputId": "af6f3d09-0cfa-45e7-c86c-45845e5227f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nLook hither he indeed,\\nThan when I saw; villain, despite of all gone:\\nBlue, if you will, sir, what's thy air?\\nHow hath remember well they thurdy youth:\\nBe it possible.\\n\\nDUKE VINCENTIO:\\nFaith, here's young skill be spoken.\\n\\nPROSPERO:\\nHe was a kind of leave, I'll carry it\\nas they are for Tybalt. Will the white robbeys may square thee hence;\\nBut all together when he will seruct their distrust,\\nResign'd and ship the swayless fury.\\n\\nMersen:\\nWhy, what should you sleep? 'tis begin ty-morn; sleep in your grace's haste.\\n\\nWARWICK:\\nAy, wherefore gentle sirs!\\n\\nGREMIO:\\nNot to know, your funches are that good deputy\\nIs by the loss of her most ears by mind; but what\\nwe repair'd the court, of an agrective prove,\\nTo subject the devil's day: if thou fled,\\nAs I disdoubt every Juliet.\\n\\nKING RICHARD II:\\nRight, you are as this young prince, and Saint A Endless,\\nWhere I have gostion of himself among these rats that makes me\\nFor the Galogs, lie himself to you,\\nTake once revenge his contraries.\\n\\nKATHARINA:\\nWh\"\n",
            " b\"ROMEO:\\nAy, may such as I course, it was a triad,\\nYet lie as lie cut for the nect thunder.\\n\\nDUCHESS OF YORK:\\nWhy, Japy bory to you, fear'd.\\nWho are of his officer, and 'tis time lived,\\nThe offences which I did? I am none. Go to;\\nFor I now mean it engeopled when:\\nYet to kingling no roots your hand of fordes,\\nwhere she profess and perpetual heart would immort\\nA giving her part of you, and see how he is come\\nI'll tell the promoking private of this begot.\\n\\nCAMILLO:\\nI'll pray no England we puss of company.\\n\\nVedow:\\nYes, Warwick, and I know not where you accould for--\\n\\nQUEEN MARGARET:\\nFrom who have part of death he's son shall please,\\nThat you might stumbled, and my wife' not light it.\\n\\nNORTHUMBERLAND:\\nThe king is not you shall. Police, I pray you?\\n\\nVolsce:\\nHe be avenged on thee! Romeo! now, I pray,\\nMy words that I'll very little obedien and dance.\\nToo lost! for she and I know of it.\\n\\nGitlos: you\\nwill not you say.\\n\\nLUCIO:\\n\\nISABELLA:\\nBe thou you hear, Marcius!\\n\\nPRINCE:\\nSay my messages, aid, perish' a\"\n",
            " b\"ROMEO:\\nWhere do, I warrant her, she's come upon them such babe\\nOf England an old cartain you, acceptles;\\nwhist all down. Dear sie, if I may content you, I do believe\\nHereafter may procures her good hosolicio,\\nVerily, I would have won. Forlow, pity,\\nIs't not conourel thee, deal-mispleces with your careless,\\nNor God, my heart, commends it then enough.\\n\\nSEBASTIAN:\\nDishonours not you first so stirr'd;\\nWhat thou hast lost your queen and being known well together,\\nAnd go with me, perpainty did before.\\n\\nKING RICHARD II:\\nGo thou to Frunce,\\nTo cross me, indeed against him: if I do not,\\nNot like to hell for blotwerness with conscience,\\nWhen I shall ask yond censuried good\\nFet forty you, for our exercise,\\nSpread full of his own sword, he was murdered like your hands.\\nI kneel to envious the champion with him,\\nAs the impeyiment of our victory,\\nOnly much I have: but he's deceived:\\nThe weeds from fools and truth earnestly begin.\\n\\nLUCENTIO:\\nTass' no more: for I do now permition with my them,\\nAnd haply moved\"\n",
            " b\"ROMEO:\\nSweet queen, pay and buzzardly; and for either thee,\\nUnless the wanton stain by, obhing eyes,\\nBe you know the last, though her behalf down ignorable!\\nCome, Warwick, now I have kept thus fired,\\nHe hath given for meat or gently of their\\neyes,\\nTo bent the face o' the poor heart\\nThe clouds were traitors\\nA great preporter that slew them to you;\\nThis did not--which have I such made in peace,\\nYour defended with a flatteries have done.\\n\\nGONZALO:\\nO, no more, no more!\\nIf he was been will own accusery for a better\\nhere that is none worn him. He is my father,\\nWhen he shall be my wife be sadify, and so attain\\nBy Clarence this; hold you our army.\\n\\nMARCIUS:\\nThe kingdom say you young.\\n\\nKING EDWARD IV:\\nSweet from the way, and give thee this.\\n\\nMIRANDA:\\nOrmempe,--\\n\\nAUFIDIUS:\\nMake heed?\\n\\nBALTHASAR:\\nI did not, sister.\\n\\nPOMPEY:\\nI shall shake him, view this waked, you would fain with hand,\\nTowards her here with thee amongst these manifest worm\\nHad been without the pedina.'\\n\\nGhORGSET:\\nA heavy child! I prithe\"\n",
            " b\"ROMEO:\\nIh something having like sorrow, repent thee,\\nWhere she consess I pind them vengeance from the water\\nUndart ye! Ho! woo't! What's heaven's most behind that I have ta'en-letting\\nhold thee cures of Somerset, thus I will die,\\nThat might I live, I'll provide your crymate down,\\nOr in his sweet woef with the head indeed\\nWhy, here I lie deadly shed thee,\\nBale conveyance on his happy; but I warrant ye,\\nThat you should show himself do an\\nadmitted fundom from my falsehood speaks of what.\\nWhat through until the all-beard partion\\nMay show the banish'd prophy sort,\\nStanding eyes, since is his angry's openies,\\nAnd raw ago forfeit.\\n\\nServant:\\nThese faith he deeds was indeed.\\n\\nANGELO:\\nWe sway, he might sit up my son!\\nKnops she's wondrous from thy presence to my soul,\\nEven to past comm wated cries And! Rascaling will\\nHave done't: I say, I cannot follow any thing;\\nAnd how you will dr'd; but we entome heart.\\n\\nNORTHUMBERLAND:\\nRichard, desires and widow spare fair Bianca:\\nAy, wilt thou deny it?\\n\\nCLARENCE:\\n\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.294729471206665\n"
          ]
        }
      ],
      "source": [
        "# Batch the inputs to create more examples effectively\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehi64ZFDr9SU",
        "outputId": "65612ff3-7890-4b4d-b43e-bc7f4f3c602b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fa380b8c290>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ],
      "source": [
        "# Save and load the trained model\n",
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0rR6rCmsSic",
        "outputId": "ab533214-6612-48f2-a3d6-16bfd8c51907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Indeed, by God's sof, sir; no.\n",
            "\n",
            "MENENIUS:\n",
            "Only of what with a bowl down yet were past\n",
            "Like an oppor\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C3-RNNShakespeareCharacter.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f8b8d46b596f3682efb9d41cbca3c28fe17fcce043b965f9dcdd46717bc9d873"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
